[%
  title='MyMediaLite: How to use the command line programs'
  prefix='../'
%]

[% INCLUDE header %]

<h1><a href="index.html">[% title %]</a></h1>

[% INCLUDE menu %]

[% INCLUDE infobox %]

<div id="content">
<p>
MyMediaLite is mainly a library, meant to be used by other applications.<br/>
We provide two command-line tools that offer much of MyMediaLite's functionality.
They allow you to find out how MyMediaLite can deal with a dataset without having
to integrate the library in your application or having to develop your own program.
</p>
<h2>Rating Prediction</h2>
<p>
  The programs expect the data to be in a simple text format:
  <pre>
  user_id item_id rating
  </pre>
  where user_id and item_id are integers referring to users and items, respectively,
  and rating is a floating-point number expressing how much a user likes an item.
  The separator between the values can be either spaces, tabs, or commas.
  If there are more than three columns, all additional columns will be ignored.

  A small example dataset:
  <pre>
  1       1       5
  1       2       3
  1       3       4
  1       4       3
  1       5       3
  1       7       4
  </pre>
  </p>

  <p>
    The general <a href="rating_prediction.html">usage of the rating prediction program</a> is as follows:
    <pre>
    rating_prediction --training-file=TRAINING_FILE --test-file=TEST_FILE --recommender=METHOD [OPTIONS]
    </pre>
    METHOD is the recommender to use, which will be trained using the contents
    of TRAINING_FILE. The recommender will then predict the data in TEST_FILE, and the program
    will display the RMSE (root mean square error) and MAE (mean absolute error) of the
    predictions.
  </p>

  <p>
    <a href="rating_prediction.html">
      If you call <tt>rating_prediction</tt> without arguments, it will provide a list of recommenders
      to choose from, plus their arguments and further options.
    </a>
  </p>

  <p>
    You can <a href="http://www.grouplens.org/node/73">download the MovieLens 100k ratings dataset from the
    GroupLens Research website</a> and unzip it to have something to play with.
    If you have downloaded the MyMediaLite source code, you can do this automatically by entering
    <pre>
      make download-movielens
    </pre>
  </p>

  <p>
    To try out a simple baseline method on the data, you just need to enter
    <pre>
    rating_prediction --training-file=u1.base --test-file=u1.test --recommender=UserAverage
    </pre>
    which should give a result like
    <pre>
    UserAverage training_time 00:00:00.000098 RMSE 1.063 MAE 0.85019 testing_time 00:00:00.032326
    </pre>
  </p>

  <p>
    To use a more advanced recommender, enter
    <pre>
    rating_prediction --training-file=u1.base --test-file=u1.test --recommender=BiasedMatrixFactorization
    </pre>
    which yields better result than the user average:
    <pre>
    BiasedMatrixFactorization num_factors=10 regularization=0.015 learn_rate=0.01 num_iter=30
                                init_mean=0 init_stdev=0.1
    training_time 00:00:03.3575780 RMSE 0.96108 MAE 0.75124 testing_time 00:00:00.0159740
    </pre>
    The key-value pairs after the method name represent arguments to the recommender
    that may be modified to get even better results. For instance, we could use more latent factors
    per user and item, which leads to a more complex (and hopefully more accurate) model:
    <pre>
    rating_prediction --training-file=u1.base --test-file=u1.test --recommender=BiasedMatrixFactorization --recommender-options="num_factors=20"
    ...
    ... RMSE 0.98029 MAE 0.76558
    </pre>
    Wait a second. The RMSE actually got worse!<br/>
    This may be because we do not train our model with the optimal arguments.
    One thing to look at is the number of iterations.
    If we iterate for too long, the learning process overfits the training data,
    which means the resulting model does not generalize well to predict unknown future data.
    The options <tt>find_iter=A</tt>, <tt>num_iter=B</tt> and <tt>max_iter=C</tt> help us
    to find the right number of iterations. Togehter, the three options mean
    &quot;From iteration B on, give out the evaluation results on the test set every A iterations,
    until you reach iteration C.&quot;
    <pre>
    rating_prediction --training-file=u1.base --test-file=u1.test --recommender=BiasedMatrixFactorization --recomender-options="num_factors=20 num_iter=0" --max-iter=25 --num-iter=0
    ...
    RMSE 1.17083 MAE 0.96918 iteration 0
    RMSE 1.01383 MAE 0.8143 iteration 1
    RMSE 0.98742 MAE 0.78742 iteration 2
    RMSE 0.97672 MAE 0.77668 iteration 3
    RMSE 0.9709 MAE 0.77078 iteration 4
    RMSE 0.96723 MAE 0.76702 iteration 5
    RMSE 0.96466 MAE 0.76442 iteration 6
    RMSE 0.96269 MAE 0.76241 iteration 7
    RMSE 0.96104 MAE 0.76069 iteration 8
    RMSE 0.95958 MAE 0.75917 iteration 9
    RMSE 0.95825 MAE 0.75783 iteration 10
    RMSE 0.95711 MAE 0.75667 iteration 11
    RMSE 0.95626 MAE 0.75569 iteration 12
    RMSE 0.95578 MAE 0.75501 iteration 13
    RMSE 0.95573 MAE 0.75467 iteration 14
    RMSE 0.95611 MAE 0.75467 iteration 15
    RMSE 0.9569 MAE 0.75499 iteration 16
    RMSE 0.95802 MAE 0.75551 iteration 17
    RMSE 0.95942 MAE 0.75623 iteration 18
    RMSE 0.96102 MAE 0.7571 iteration 19
    RMSE 0.96277 MAE 0.75806 iteration 20
    RMSE 0.96463 MAE 0.75909 iteration 21
    RMSE 0.96656 MAE 0.76017 iteration 22
    RMSE 0.96852 MAE 0.7613 iteration 23
    RMSE 0.9705 MAE 0.76246 iteration 24
    RMSE 0.97247 MAE 0.76364 iteration 25
    </pre>
    This means that we should probably set <tt>--num-iter</tt> to around 15 to get better results.
  </p>

  <p>
    <b>Warning:</b>
    Depending on the recommender, the choice of arguments (hyperparameters) may be crucial to
    the recommender's performance.
    Sometimes you may find suitable values by playing a bit with the arguments, starting from their default
    values. However, there is no guarantee that this will work!
    You should use a principled approach to find good hyperparameters, e.g. cross-validation and grid search.
  </p>

  <!-- TODO document load/save -->
  <!-- TODO document cutoff techniques -->

<h2>Item Prediction from Positive-Only Feedback</h2>
<p>
  The item recommendation program behaves similarly to the rating prediction program,
  so we concentrate on the differences here.
</p>

<p>
  <pre>item_recommendation --training-file=TRAINING_FILE --test-file=TEST_FILE --recommender=METHOD [OPTIONS]</pre>
</p>

<p>
  Again,
  <a href="item_prediction.html">
    if you call <tt>item_recommendation</tt> without arguments, it will provide a list of recommenders
    to choose from, plus their arguments and further options.
  </a>
</p>

<p>
  The third column in the data files may be omitted. If it is present, it will be ignored.
</p>

<p>
  Example:
  <pre>
  1       1
  1       2
  1       3
  1       4
  1       5
  1       7
  </pre>
</p>

  <p>
  Instead of RMSE and MAP, the evaluation measures are now prec@N (precision at N), <a href="http://en.wikipedia.org/wiki/Receiver_operating_characteristic">AUC (area under the ROC curve)</a>,
  MAP (mean average precision), and <a href="http://en.wikipedia.org/wiki/Discounted_cumulative_gain">NDCG (normalized discounted cumulative gain)</a>.
  </p>

  <p>
  Let us start again with some baseline methods, <tt>Random</tt> and <tt>MostPopular</tt>:
  <pre>
  item_recommendation --training-file=u1.base --test-file=u1.test --recommender=Random
  random training_time 00:00:00.0001040
    AUC 0.49924 prec@5 0.02789 prec@10 0.02898 MAP 0.00122 NDCG 0.37214
    num_users 459 num_items 1650 testing_time 00:00:02.7115540

  item_recommendation --training-file=u1.base --test-file=u1.test --recommender=MostPopular
  MostPopular training_time 00:00:00.0015710
    AUC 0.8543 prec@5 0.322 prec@10 0.30458 MAP 0.02187 NDCG 0.57038
    num_users 459 num_items 1650 testing_time 00:00:02.3813790
  </pre>
  </p>

  <p>
  User-based collaborative filtering:
  <pre>
  item_recommendation --training-file=u1.base --test-file=u1.test --recommender=UserKNN
  UserKNN k=80 training_time 00:00:05.6057200
    AUC 0.91682 prec@5 0.52505 prec@10 0.46776 MAP 0.06482 NDCG 0.68793
    num_users 459 num_items 1650 testing_time 00:00:08.8362840
  </pre>
  </p>

  <p>
  Note that item recommendation evaluation usually takes longer than the rating prediction evaluation,
  because for each user, scores for every candidate item (possibly all items) have to be computed.
  You can restrict the number of predictions to be made using the options <tt>--test-users=FILE</tt>
  and <tt>--candidate-items=FILE</tt> to save time.
  </p>

  <p>
  The item recommendation program supports the same options (<tt>--find-iter=N</tt> etc.)
  for iteratively trained recommenders like <tt>BPRMF</tt> and <tt>WRMF</tt>.
  </p>

</div>

[% INCLUDE footer %]
